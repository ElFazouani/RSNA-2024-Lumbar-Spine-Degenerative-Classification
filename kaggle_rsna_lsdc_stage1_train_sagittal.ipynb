{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h71KTJXbS_Tz",
    "outputId": "a2f43517-7fb5-4b8b-de22-75279cd573c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import pydicom\n",
    "\n",
    "import timm\n",
    "from transformers import RobertaPreLayerNormConfig, RobertaPreLayerNormModel\n",
    "\n",
    "\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "from segmentation_models_pytorch.decoders.unet.model import (\n",
    "    UnetDecoder,\n",
    "    SegmentationHead,\n",
    ")\n",
    "\n",
    "import monai.transforms as transforms\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "DATA_DIR = 'input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5O3mtxnTC6E"
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyjkKpVuTDyi"
   },
   "outputs": [],
   "source": [
    "class CustomConfig:\n",
    "    seed = 42\n",
    "    device = 'cuda'\n",
    "    root = DATA_DIR\n",
    "\n",
    "    train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "    label_columns = train.columns[1:]\n",
    "\n",
    "    orientation = ['Sagittal T2/STIR', 'Sagittal T1'][1]\n",
    "\n",
    "    if orientation == 'Sagittal T2/STIR':\n",
    "        label_columns = label_columns[0:5]\n",
    "    elif orientation == 'Sagittal T1':\n",
    "        label_columns = label_columns[5:15]\n",
    "    elif orientation == 'Axial T2':\n",
    "        label_columns = label_columns[15:25]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    volume_size = {\n",
    "        'Sagittal T2/STIR' : [29, 256, 256],\n",
    "        'Sagittal T1' : [38, 256, 256],\n",
    "        'Axial T2' : [192, 256, 256]\n",
    "    }[orientation]\n",
    "\n",
    "    sigma = {\n",
    "        'Sagittal T2/STIR' : 4,\n",
    "        'Sagittal T1' : 4,\n",
    "        'Axial T2' : 4\n",
    "    }[orientation]\n",
    "\n",
    "    n_fold = 5\n",
    "\n",
    "    batch_size = 2\n",
    "    n_worker = 16\n",
    "\n",
    "    lr = 1e-4\n",
    "    wd = 1e-2\n",
    "    n_epoch = 40\n",
    "    warmup_ratio = 0.1\n",
    "    test_freq = 1\n",
    "    mix_prob = 0.0\n",
    "\n",
    "    coordinate_columns = {\n",
    "        'Sagittal T2/STIR' : [\n",
    "            ['Spinal Canal Stenosis', 'L1/L2'],\n",
    "            ['Spinal Canal Stenosis', 'L2/L3'],\n",
    "            ['Spinal Canal Stenosis', 'L3/L4'],\n",
    "            ['Spinal Canal Stenosis', 'L4/L5'],\n",
    "            ['Spinal Canal Stenosis', 'L5/S1'],\n",
    "        ],\n",
    "        'Sagittal T1' : [\n",
    "            ['Left Neural Foraminal Narrowing', 'L1/L2'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L2/L3'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L3/L4'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L4/L5'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L5/S1'],\n",
    "\n",
    "            ['Right Neural Foraminal Narrowing', 'L1/L2'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L2/L3'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L3/L4'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L4/L5'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L5/S1'],\n",
    "        ],\n",
    "        'Axial T2' : [\n",
    "            ['Left Subarticular Stenosis', 'L1/L2'],\n",
    "            ['Left Subarticular Stenosis', 'L2/L3'],\n",
    "            ['Left Subarticular Stenosis', 'L3/L4'],\n",
    "            ['Left Subarticular Stenosis', 'L4/L5'],\n",
    "            ['Left Subarticular Stenosis', 'L5/S1'],\n",
    "\n",
    "            ['Right Subarticular Stenosis', 'L1/L2'],\n",
    "            ['Right Subarticular Stenosis', 'L2/L3'],\n",
    "            ['Right Subarticular Stenosis', 'L3/L4'],\n",
    "            ['Right Subarticular Stenosis', 'L4/L5'],\n",
    "            ['Right Subarticular Stenosis', 'L5/S1'],\n",
    "        ],\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = CustomConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpOm4ClDTQuy"
   },
   "source": [
    "## seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koQGTWpzTR1R"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaQHZXhobtNc"
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzq1ZnXebuIV"
   },
   "outputs": [],
   "source": [
    "def volume2point(volumes):\n",
    "    volumes = volumes.cpu()\n",
    "\n",
    "    batch_size, n_class, _, h, w = volumes.shape\n",
    "\n",
    "    points = []\n",
    "    for i in range(batch_size):\n",
    "        volume = volumes[i]\n",
    "\n",
    "        point = []\n",
    "        for j in range(n_class):\n",
    "            flat_volume = volume[j].reshape(-1)\n",
    "            max_idx = torch.argmax(flat_volume)\n",
    "\n",
    "            z = max_idx // (h * w)\n",
    "            y = (max_idx % (h * w)) // w\n",
    "            x = (max_idx % (h * w)) % w\n",
    "            point.append(torch.stack([z, y, x]))\n",
    "\n",
    "        point = torch.stack(point, dim = 0)\n",
    "        points.append(point)\n",
    "\n",
    "    points = torch.stack(points, dim = 0)\n",
    "    return points.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOaNHqoITdt4"
   },
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFILGbcgTfSs"
   },
   "outputs": [],
   "source": [
    "def preprocess(args):\n",
    "    train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    kf = KFold(n_splits = args.n_fold, shuffle = True, random_state = args.seed)\n",
    "    for train_indices, test_indices in kf.split(train):\n",
    "\n",
    "        train_df = train.loc[train_indices].reset_index(drop = True)\n",
    "        test_df = train.loc[test_indices].reset_index(drop = True)\n",
    "\n",
    "        folds.append([train_df, test_df])\n",
    "\n",
    "    return train, folds\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train, folds = preprocess(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OjRy4UtNYAR"
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2vUX6rSy26_"
   },
   "outputs": [],
   "source": [
    "class CustomTransform(nn.Module):\n",
    "    def __init__(self, args, train = False, volume_size = None):\n",
    "        super(CustomTransform, self).__init__()\n",
    "\n",
    "        if train:\n",
    "            self.transform = A.Compose([\n",
    "                A.ShiftScaleRotate(border_mode = cv2.BORDER_CONSTANT, p = 0.75),\n",
    "                A.RandomBrightnessContrast(p = 0.75),\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "            ])\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        c, z, h, w = y.shape\n",
    "\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = x.numpy()\n",
    "\n",
    "        y = y.reshape(c * z, h, w)\n",
    "        y = y.permute(1, 2, 0)\n",
    "        y = y.numpy()\n",
    "\n",
    "        results = self.transform(image = x, mask = y)\n",
    "        x, y = results['image'], results['mask']\n",
    "\n",
    "        x = torch.tensor(x, dtype = torch.float)\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        y = torch.tensor(y, dtype = torch.float)\n",
    "        y = y.permute(2, 0, 1)\n",
    "\n",
    "        _, h, w = y.shape\n",
    "        y = y.reshape(c, z, h, w)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "j3rcd_0IzGPK",
    "outputId": "b8426be4-01c6-4322-fc81-72d71de34291"
   },
   "outputs": [],
   "source": [
    "def convert_to_8bit(x):\n",
    "    lower, upper = np.percentile(x, (1, 99))\n",
    "    x = np.clip(x, lower, upper)\n",
    "    x = x - np.min(x)\n",
    "    x = x / np.max(x)\n",
    "    return (x * 255).astype(\"uint8\")\n",
    "\n",
    "def get_imgs(dcms):\n",
    "    imgs = []\n",
    "    for dcm in dcms:\n",
    "        img = convert_to_8bit(dcm.pixel_array)\n",
    "        imgs.append(img)\n",
    "    try:\n",
    "        return np.stack(imgs, axis = 0)\n",
    "    except:\n",
    "        h, w = imgs[0].shape\n",
    "        imgs = [A.Resize(h, w)(image = x)['image'] for x in imgs]\n",
    "        return np.stack(imgs, axis = 0)\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, df, train = False):\n",
    "        self.args = args\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "\n",
    "        self.orientation = args.orientation\n",
    "        self.volume_size = args.volume_size\n",
    "\n",
    "        self.transform = CustomTransform(args, self.train, self.volume_size)\n",
    "\n",
    "        self.train_series_descriptions = pd.read_csv(f'{DATA_DIR}/train_series_descriptions.csv')\n",
    "        self.train_label_coordinates = pd.read_csv(f'{DATA_DIR}/coords_rsna_improved.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_inputs(self, row):\n",
    "        row_series_descriptions = self.train_series_descriptions[self.train_series_descriptions['study_id'] == row['study_id']]\n",
    "\n",
    "        if self.orientation in list(row_series_descriptions['series_description']):\n",
    "            study_id, series_id, _ = row_series_descriptions[row_series_descriptions['series_description'] == self.orientation].reset_index(drop = True).loc[0]\n",
    "            dicoms = sorted(glob.glob(f'{DATA_DIR}/train_images/{study_id}/{series_id}/*.dcm'), key = lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            dicoms = [pydicom.dcmread(x) for x in dicoms]\n",
    "\n",
    "            if 'Sagittal' in self.orientation:\n",
    "                pos = np.asarray([d.ImagePositionPatient for d in dicoms])[:, 0]\n",
    "            else:\n",
    "                pos = np.asarray([d.ImagePositionPatient for d in dicoms])[:, -1]\n",
    "\n",
    "            inputs = get_imgs(dicoms)\n",
    "\n",
    "        else:\n",
    "            inputs = np.zeros(self.volume_size, dtype = np.float32)\n",
    "            pos = np.zeros([self.volume_size[0]], dtype = np.float32)\n",
    "\n",
    "        inputs = A.Resize(self.volume_size[1], self.volume_size[2])(image = inputs.transpose(1, 2, 0))['image'].transpose(2, 0, 1)\n",
    "        inputs = torch.tensor(inputs, dtype = torch.float)\n",
    "        inputs = inputs / 255.0\n",
    "        return inputs, pos\n",
    "\n",
    "    def get_targets(self, row, inputs):\n",
    "        row_series_descriptions = self.train_series_descriptions[self.train_series_descriptions['study_id'] == row['study_id']]\n",
    "\n",
    "        targets = np.zeros([len(self.args.coordinate_columns[self.orientation])] + [inputs.shape[0]] + self.volume_size[1:], dtype = np.float32)\n",
    "\n",
    "        if self.orientation in list(row_series_descriptions['series_description']):\n",
    "            study_id, series_id, _ = row_series_descriptions[row_series_descriptions['series_description'] == self.orientation].reset_index(drop = True).loc[0]\n",
    "\n",
    "            dicoms = sorted(glob.glob(f'{DATA_DIR}/train_images/{study_id}/{series_id}/*.dcm'), key = lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            instance_numbers = [int(x.split('/')[-1].split('.')[0]) for x in dicoms]\n",
    "            instance2index = dict(zip(instance_numbers, [i for i in range(len(instance_numbers))]))\n",
    "\n",
    "            label_coordinate = self.train_label_coordinates[(self.train_label_coordinates['study_id'] == study_id) & (self.train_label_coordinates['series_id'] == series_id)].reset_index(drop = True)\n",
    "            for i in range(len(self.args.coordinate_columns[self.orientation])):\n",
    "                try:\n",
    "                    condition, level = self.args.coordinate_columns[self.orientation][i][0], self.args.coordinate_columns[self.orientation][i][1]\n",
    "                    if self.orientation == 'Sagittal T2/STIR':\n",
    "                        _, _, x, y, _, _, instance_number, _, _,  = label_coordinate[(label_coordinate['condition'] == condition) & (label_coordinate['level'] == level) & (label_coordinate['side'] == 'R')].reset_index(drop = True).loc[0]\n",
    "                    else:\n",
    "                        _, _, x, y, _, _, instance_number, _, _,  = label_coordinate[(label_coordinate['condition'] == condition) & (label_coordinate['level'] == level)].reset_index(drop = True).loc[0]\n",
    "                    h, w = pydicom.dcmread(f'{DATA_DIR}/train_images/{study_id}/{series_id}/{instance_number}.dcm').pixel_array.shape\n",
    "                    z = instance2index[instance_number]\n",
    "\n",
    "                    z_i = z\n",
    "                    y_i = round(y * (self.volume_size[1]))\n",
    "                    x_i = round(x * (self.volume_size[2]))\n",
    "\n",
    "                    targets[i, z_i, y_i, x_i] = 1\n",
    "\n",
    "                    targets[i, z_i] = gaussian_filter(targets[i, z_i], sigma = (self.args.sigma, self.args.sigma))\n",
    "                    targets[i, z_i] = targets[i, z_i] / targets[i, z_i].max()\n",
    "\n",
    "                    targets[i, max(0, z_i - 2)] = targets[i, z_i] / 2**2\n",
    "                    targets[i, max(0, z_i - 1)] = targets[i, z_i] / 2**1\n",
    "\n",
    "                    targets[i, min(targets.shape[1] - 1, z_i + 2)] = targets[i, z_i] / 2**2\n",
    "                    targets[i, min(targets.shape[1] - 1, z_i + 1)] = targets[i, z_i] / 2**1\n",
    "\n",
    "                except: pass\n",
    "\n",
    "        targets = torch.tensor(targets, dtype = torch.float)\n",
    "        return targets\n",
    "\n",
    "\n",
    "    def get_masks(self, inputs, targets):\n",
    "        temporal_masks = (inputs.sum(dim = [1, 2]) != 0).float()\n",
    "        spatial_masks = (targets.sum(dim = [1, 2, 3]) != 0).float()\n",
    "\n",
    "        masks = temporal_masks[None, :] * spatial_masks[:, None]\n",
    "        return masks\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.loc[index]\n",
    "        inputs, pos = self.get_inputs(row)\n",
    "        targets = self.get_targets(row, inputs)\n",
    "\n",
    "        pos = np.argsort(pos)\n",
    "\n",
    "        inputs = inputs[pos]\n",
    "        targets = targets[:, pos]\n",
    "\n",
    "        if inputs.shape[0] > self.volume_size[0]:\n",
    "            inputs = F.interpolate(inputs.unsqueeze(0).unsqueeze(0), size = self.volume_size, mode = 'trilinear').squeeze(0).squeeze(0)\n",
    "            targets = F.interpolate(targets.unsqueeze(0), size = self.volume_size, mode = 'trilinear').squeeze(0)\n",
    "\n",
    "        inputs, targets = self.transform(inputs, targets)\n",
    "\n",
    "        inputs = torch.cat([inputs, torch.zeros([self.volume_size[0] - inputs.shape[0]] + list(inputs.shape[1:]), dtype = torch.float)], dim = 0)\n",
    "        targets = torch.cat([targets, torch.zeros([len(self.args.coordinate_columns[self.orientation])] + [self.volume_size[0] - targets.shape[1]] + list(inputs.shape[1:]), dtype = torch.float)], dim = 1)\n",
    "\n",
    "        masks = self.get_masks(inputs, targets)\n",
    "        return inputs, targets, masks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_df, test_df = folds[0]\n",
    "    dataset = CustomDataset(args, train_df, train = True)\n",
    "\n",
    "    index = random.randint(0, len(dataset) - 1)\n",
    "    inputs, targets, masks = dataset[index]\n",
    "\n",
    "    print('inputs : ', inputs.shape)\n",
    "    print('targets : ', targets.shape)\n",
    "    print('masks : ', masks.shape)\n",
    "\n",
    "    print(volume2point(targets[None]))\n",
    "\n",
    "    fig, axes = plt.subplots(2, targets[0].shape[0], figsize = (5 * targets[0].shape[0], 5 * 2))\n",
    "    for i in range(targets[0].shape[0]):\n",
    "        axes[0, i].imshow(inputs[i], cmap = 'gray')\n",
    "        axes[1, i].imshow(targets[0][i], cmap = 'gray')\n",
    "    plt.show()\n",
    "    plt.imshow(targets[0].permute(1, 0, 2).reshape(256, -1))\n",
    "    plt.show()\n",
    "\n",
    "    patch_size = {'Sagittal T2/STIR' : 32, 'Sagittal T1' : 32, 'Axial T2' : 32}[args.orientation]\n",
    "    fig, axes = plt.subplots(2, targets.shape[0], figsize = (5 * targets.shape[0], 5 * 2))\n",
    "\n",
    "    points = volume2point(targets[None])[0]\n",
    "    for i in range(targets.shape[0]):\n",
    "        z, y, x = points[i]\n",
    "        if points[i].sum() != 0:\n",
    "            axes[0, i].imshow(inputs[z, y-patch_size:y+patch_size, x-patch_size:x+patch_size], cmap = 'gray')\n",
    "            axes[1, i].imshow(targets[i][z, y-patch_size:y+patch_size, x-patch_size:x+patch_size], cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQo9CkvsVGWt"
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIZrthK4VGWu"
   },
   "outputs": [],
   "source": [
    "class Segmenter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 n_channel,\n",
    "                 n_blocks = 4\n",
    "                 ):\n",
    "        super(Segmenter, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.n_class = len(args.label_columns)\n",
    "        self.n_channel = n_channel\n",
    "\n",
    "        self.extracter = timm.create_model('tf_efficientnet_b5_ns', \n",
    "                              pretrained=True,\n",
    "                              features_only = True,\n",
    "                              out_indices=range(n_blocks),\n",
    "                                      )\n",
    "\n",
    "        encoder_channels = [n_channel] + [self.extracter.feature_info[i][\"num_chs\"] for i in range(n_blocks)]\n",
    "        decoder_channels = [256, 128, 64, 32, 16, 8][:n_blocks]\n",
    "\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels = encoder_channels,\n",
    "            decoder_channels = decoder_channels,\n",
    "            n_blocks = n_blocks,\n",
    "            use_batchnorm = True,\n",
    "            center = False,\n",
    "            attention_type = None,\n",
    "        )\n",
    "\n",
    "        self.head = SegmentationHead(\n",
    "            in_channels = decoder_channels[-1],\n",
    "            out_channels = self.n_class,\n",
    "            activation = None,\n",
    "            kernel_size= 3,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.n_channel, self.args.volume_size[1], self.args.volume_size[2])\n",
    "        _x = self.extracter(x)\n",
    "\n",
    "        x = self.decoder(*[x] + _x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        x = x.reshape(-1, self.args.volume_size[0], self.n_class, self.args.volume_size[1], self.args.volume_size[2])\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MW1KLgRPVGWv"
   },
   "outputs": [],
   "source": [
    "class Pointer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 n_channel,\n",
    "                 hidden_size,\n",
    "                 drop_rate,\n",
    "                 ):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.n_channel = n_channel\n",
    "        self.extractor = timm.create_model('regnety_002', \n",
    "                              pretrained=True,\n",
    "                              features_only = True,\n",
    "                                          )\n",
    "\n",
    "        self.dense_size = self.extractor.feature_info[-1][\"num_chs\"]\n",
    "        \n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = self.dense_size,\n",
    "            hidden_size = hidden_size,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "            )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Dropout(p = drop_rate),\n",
    "            nn.Linear(2 * hidden_size, len(args.label_columns))\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x.reshape(-1, self.n_channel, self.args.volume_size[1], self.args.volume_size[2])\n",
    "        x = self.extractor(x)\n",
    "        x = x[-1].mean(dim = [2, 3])\n",
    "        \n",
    "        x = x.reshape(-1, self.args.volume_size[0], self.dense_size)\n",
    "        x, _ =  self.rnn(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9CcAZtxVGWw",
    "outputId": "47a504eb-a018-40ed-b568-c33b4b265781"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 n_channel = 3,\n",
    "                 hidden_size = 256,\n",
    "                 drop_rate = 0.3,\n",
    "                 ):\n",
    "\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.n_channel = n_channel\n",
    "\n",
    "        self.segmenter = Segmenter(\n",
    "            args = args,\n",
    "            n_channel = n_channel,\n",
    "            )\n",
    "\n",
    "        self.pointer = Pointer(\n",
    "            args = args,\n",
    "            n_channel = n_channel,\n",
    "            hidden_size = hidden_size,\n",
    "            drop_rate = drop_rate,\n",
    "        )\n",
    "\n",
    "    def get_inputs(self, x):\n",
    "        x = F.pad(x, (0, 0, 0, 0, (self.n_channel-1)//2, (self.n_channel-1)//2), \"constant\", 0)\n",
    "        x = [x[:, i:i+self.n_channel] for i in range(self.args.volume_size[0])]\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    def get_masks(self, x):\n",
    "        x = (x.sum(dim = [2, 3]) != 0).float()\n",
    "        return x\n",
    "\n",
    "    def get_outputs(self, x1, x2, mask):\n",
    "        x1 = torch.sigmoid(x1)\n",
    "        x2 = torch.sigmoid(x2)\n",
    "\n",
    "        x = x1 * x2[:, :, :, None, None] * mask[:, None, :, None, None]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.get_masks(x)\n",
    "\n",
    "        x = self.get_inputs(x)\n",
    "\n",
    "        x1 = self.segmenter(x)\n",
    "\n",
    "        x2 = self.pointer(x, mask)\n",
    "\n",
    "        return x1, x2, self.get_outputs(x1, x2, mask)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
    "    sample = next(iter(loader))\n",
    "    sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "    model = CustomModel(args)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1, outputs2, outputs = model(sample[0])\n",
    "        print(outputs1.shape)\n",
    "        print(outputs2.shape)\n",
    "        print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7qXKdS9dc0c"
   },
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aK0Fwcv2reoK"
   },
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction = 'none')\n",
    "\n",
    "\n",
    "    def get_loss1(self, preds, trues, masks):\n",
    "        loss = self.bce_loss(preds.float(), trues.float())\n",
    "        loss = loss.mean(dim = [3, 4])\n",
    "        loss = (loss * masks).sum() / masks.sum()\n",
    "        return loss\n",
    "\n",
    "    def get_loss2(self, preds, trues, masks):\n",
    "        trues = trues.amax(dim = [3, 4])\n",
    "        loss = self.bce_loss(preds, trues)\n",
    "        loss = (loss * masks).sum() / masks.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, preds1, preds2, trues, masks):\n",
    "        loss1 = self.get_loss1(preds1, trues, masks)\n",
    "        loss2 = self.get_loss2(preds2, trues, masks)\n",
    "\n",
    "        loss = (loss1 + loss2)\n",
    "        return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
    "    sample = next(iter(loader))\n",
    "    sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "    model = CustomModel(args)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    loss_fn = CustomLoss(args)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1, outputs2, _ = model(sample[0])\n",
    "        loss = loss_fn(outputs1, outputs2, sample[1], sample[2])\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l73jFOxU3_d"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChTYtMba6uHN"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer:\n",
    "    def __init__(self, args, model):\n",
    "        self.model = model\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "\n",
    "        self.log_path = f'{args.save_dir}/log.txt'\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr = args.lr, weight_decay = args.wd)\n",
    "\n",
    "        total_steps = args.total_steps\n",
    "        warmup_steps = int(total_steps * args.warmup_ratio)\n",
    "        print('total_steps: ', total_steps)\n",
    "        print('warmup_steps: ', warmup_steps)\n",
    "\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(self.optimizer,\n",
    "                                                         num_warmup_steps = warmup_steps,\n",
    "                                                         num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "        self.train_loss_fn = CustomLoss(args)\n",
    "        self.test_loss_fn = CustomLoss(args)\n",
    "\n",
    "    def run(self, args, train_loader, test_loader):\n",
    "        for epoch in range(args.n_epoch):\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            train_loss = self.train_function(args, train_loader)\n",
    "\n",
    "            train_log = f'epoch : {epoch + 1}, lr : {lr}, train_loss: {train_loss:.6f}'\n",
    "            self.log(args, train_log)\n",
    "\n",
    "            if ((epoch + 1) % args.test_freq) == 0:\n",
    "                test_loss, score_dicts = self.test_function(args, test_loader)\n",
    "\n",
    "                test_score = score_dicts['zyx mean difference']\n",
    "\n",
    "                test_log = f'epoch : {epoch + 1}, lr : {lr}, test_loss: {test_loss:.6f}, test_score: {test_score:.6f}\\n'\n",
    "                self.log(args, test_log)\n",
    "\n",
    "                score_log = json.dumps(score_dicts, indent = 4)\n",
    "                self.log(args, score_log)\n",
    "\n",
    "                save_path = args.save_dir + '/epoch' + f'{epoch + 1}'.zfill(3) + \\\n",
    "                            f'-trainloss{round(train_loss, 6)}' + \\\n",
    "                            f'-testloss{round(test_loss, 6)}' + \\\n",
    "                            f'-testscore{round(test_score, 6)}' + '.bin'\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    def train_function(self, args, loader):\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for bi, sample in enumerate(tqdm(loader)):\n",
    "            sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "            inputs = sample[0]\n",
    "            targets = sample[1]\n",
    "            masks = sample[2]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(args.device):\n",
    "                outputs1, outputs2, _ = self.model(inputs)\n",
    "\n",
    "            loss = self.train_loss_fn(outputs1, outputs2, targets, masks)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            total_loss += loss.detach().cpu().tolist()\n",
    "\n",
    "        return total_loss/len(loader)\n",
    "\n",
    "    def test_function(self, args, loader):\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        preds = np.zeros([len(loader.dataset.df), len(args.label_columns), 3], dtype = int)\n",
    "        trues = np.zeros([len(loader.dataset.df), len(args.label_columns), 3], dtype = int)\n",
    "        masks = np.zeros([len(loader.dataset.df), len(args.label_columns)], dtype = int)\n",
    "        for bi, sample in enumerate(tqdm(loader)):\n",
    "            sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "            inputs = sample[0]\n",
    "            targets = sample[1]\n",
    "            _masks = sample[2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs1, outputs2, outputs = self.model(inputs)\n",
    "                loss = self.test_loss_fn(outputs1, outputs2, targets, _masks)\n",
    "\n",
    "            total_loss += loss.detach().cpu().tolist()\n",
    "\n",
    "            preds[(bi)*args.batch_size:(bi + 1)*args.batch_size, :, :] = volume2point(outputs)\n",
    "            trues[(bi)*args.batch_size:(bi + 1)*args.batch_size, :, :] = volume2point(targets)\n",
    "            masks[(bi)*args.batch_size:(bi + 1)*args.batch_size, :] = (_masks.sum(2) != 0).long().cpu().numpy()\n",
    "\n",
    "        pred = volume2point(outputs[0][None])[0]\n",
    "        true = volume2point(targets[0][None])[0]\n",
    "        print('pred : ', pred)\n",
    "        print('true : ', true)\n",
    "\n",
    "        outputs = outputs[0].cpu()\n",
    "        targets = targets[0].cpu()\n",
    "\n",
    "        fig, axes = plt.subplots(true.shape[0], 2, figsize = (5 * 2, 5 * true.shape[0]))\n",
    "        for i in range(true.shape[0]):\n",
    "            z, y, x = true[i]\n",
    "            axes[i, 0].imshow(outputs[i][z], cmap = 'gray')\n",
    "            axes[i, 1].imshow(targets[i][z], cmap = 'gray')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(targets[0].permute(1, 0, 2).reshape(256, -1));plt.show()\n",
    "        plt.imshow(outputs[0].permute(1, 0, 2).reshape(256, -1));plt.show()\n",
    "\n",
    "        errors = np.abs(preds - trues)\n",
    "\n",
    "        z_diff = (errors[:, :, 0] * masks).sum() / masks.sum()\n",
    "        y_diff = (errors[:, :, 1] * masks).sum() / masks.sum()\n",
    "        x_diff = (errors[:, :, 2] * masks).sum() / masks.sum()\n",
    "\n",
    "        distance = (np.sqrt((errors ** 2).sum(2)) * masks).sum() / masks.sum()\n",
    "\n",
    "        score_dicts = {\n",
    "            'zyx mean difference' : (z_diff + y_diff + x_diff) / 3,\n",
    "            'z-axis difference' : z_diff,\n",
    "            'y-axis difference' : y_diff,\n",
    "            'x-axis difference' : x_diff,\n",
    "            'distance' : distance\n",
    "        }\n",
    "        return total_loss/len(loader), score_dicts\n",
    "\n",
    "    def log(self, args, message):\n",
    "        print(message)\n",
    "        with open(f'{args.save_dir}/log.txt', 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGIGW5lKU5CF"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1cnV2LZgU6G5"
   },
   "outputs": [],
   "source": [
    "for ORIENTATION in ['Sagittal T2/STIR', 'Sagittal T1']:\n",
    "    \n",
    "    CustomConfig.orientation = ORIENTATION\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        args = CustomConfig()\n",
    "\n",
    "        args.test_freq = 5\n",
    "\n",
    "        train, folds = preprocess(args)\n",
    "\n",
    "        for i in range(args.n_fold):\n",
    "            seed_everything(args.seed)\n",
    "\n",
    "            train_df, test_df = folds[i]\n",
    "\n",
    "\n",
    "            train_dataset = CustomDataset(args, train_df, train = True)\n",
    "            test_dataset = CustomDataset(args, test_df, train = False)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                       batch_size = args.batch_size,\n",
    "                                                       num_workers = args.n_worker,\n",
    "                                                       shuffle = True,\n",
    "                                                       drop_last = True)\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                      batch_size = args.batch_size,\n",
    "                                                      num_workers = args.n_worker,\n",
    "                                                      shuffle = True,\n",
    "                                                      drop_last = False)\n",
    "\n",
    "            model = CustomModel(args)\n",
    "            model = model.to(args.device)\n",
    "\n",
    "            save_name = {\n",
    "                'Sagittal T2/STIR' : f'sagittal_t2',\n",
    "                'Sagittal T1' : f'sagittal_t1',\n",
    "                'Axial T2' : f'axial_t2',\n",
    "            }[args.orientation]\n",
    "\n",
    "            name = f'stage1/{save_name}'\n",
    "            args.save_dir = name + f'/fold{i+1}'\n",
    "            args.total_steps = int(len(train_df) * args.n_epoch/(args.batch_size))\n",
    "\n",
    "            trainer = CustomTrainer(args, model)\n",
    "            trainer.run(args, train_loader, test_loader)\n",
    "\n",
    "            del train_dataset, test_dataset, train_loader, test_loader, model, trainer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DtHBo3Xv7gnT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OgsKxoKnbsc_",
    "Mj3ahmv2bniB",
    "LfdZdnCESz3o",
    "JpOm4ClDTQuy",
    "yaQHZXhobtNc",
    "iOaNHqoITdt4",
    "h7qXKdS9dc0c"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
