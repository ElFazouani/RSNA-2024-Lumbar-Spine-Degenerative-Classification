{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h71KTJXbS_Tz",
    "outputId": "fd73d6d3-4816-468f-a283-685de51d1376"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import pydicom\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "import timm\n",
    "import gc\n",
    "from transformers import RobertaPreLayerNormConfig, RobertaPreLayerNormModel\n",
    "\n",
    "\n",
    "\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from segmentation_models_pytorch.decoders.unet.model import (\n",
    "    UnetDecoder,\n",
    "    SegmentationHead,\n",
    ")\n",
    "DATA_DIR = 'input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5O3mtxnTC6E"
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyjkKpVuTDyi"
   },
   "outputs": [],
   "source": [
    "class CustomConfig:\n",
    "    seed = 42\n",
    "    device = 'cuda'\n",
    "    root = DATA_DIR\n",
    "\n",
    "    train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "    label_columns = train.columns[1:]\n",
    "\n",
    "    ignore_index = -100\n",
    "    label2index = {'Normal/Mild' : 0, 'Moderate' : 1, 'Severe' : 2, 'Nan' : ignore_index,}\n",
    "    label2onehot = {'Normal/Mild' : [1, 0, 0], 'Moderate' : [0, 1, 0], 'Severe' : [0, 0, 1], 'Nan' : [ignore_index] * 3,}\n",
    "    label2weight = {'Normal/Mild' : 1, 'Moderate' : 2, 'Severe' : 4, 'Nan' : 0,}\n",
    "\n",
    "    scan_orientations = ['Sagittal T2/STIR', 'Sagittal T1', 'Axial T2']\n",
    "\n",
    "    n_class = 3\n",
    "    n_fold = 5\n",
    "    \n",
    "    version = 1\n",
    "\n",
    "    batch_size = 4\n",
    "    n_worker = 64\n",
    "    iters_to_accumulate = 1\n",
    "\n",
    "    lr = 1e-4\n",
    "    wd = 1e-2\n",
    "    n_epoch = 20\n",
    "    warmup_ratio = 0.1\n",
    "    test_freq = 1\n",
    "\n",
    "    mix_method = 'cutmix'\n",
    "    mix_prob = 1.0#0.5\n",
    "    mix_alpha = 4.0\n",
    "\n",
    "    patch_size = 32\n",
    "    image_size = 128\n",
    "    depth_size = 1 + 2*2\n",
    "\n",
    "    weight_loss = True\n",
    "    any_loss = False\n",
    "\n",
    "    coordinate_columns = {\n",
    "        'Sagittal T2/STIR' : [\n",
    "            ['Spinal Canal Stenosis', 'L1/L2'],\n",
    "            ['Spinal Canal Stenosis', 'L2/L3'],\n",
    "            ['Spinal Canal Stenosis', 'L3/L4'],\n",
    "            ['Spinal Canal Stenosis', 'L4/L5'],\n",
    "            ['Spinal Canal Stenosis', 'L5/S1'],\n",
    "        ],\n",
    "        'Sagittal T1' : [\n",
    "            ['Left Neural Foraminal Narrowing', 'L1/L2'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L2/L3'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L3/L4'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L4/L5'],\n",
    "            ['Left Neural Foraminal Narrowing', 'L5/S1'],\n",
    "\n",
    "            ['Right Neural Foraminal Narrowing', 'L1/L2'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L2/L3'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L3/L4'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L4/L5'],\n",
    "            ['Right Neural Foraminal Narrowing', 'L5/S1'],\n",
    "        ],\n",
    "        'Axial T2' : [\n",
    "            ['Left Subarticular Stenosis', 'L1/L2'],\n",
    "            ['Left Subarticular Stenosis', 'L2/L3'],\n",
    "            ['Left Subarticular Stenosis', 'L3/L4'],\n",
    "            ['Left Subarticular Stenosis', 'L4/L5'],\n",
    "            ['Left Subarticular Stenosis', 'L5/S1'],\n",
    "\n",
    "            ['Right Subarticular Stenosis', 'L1/L2'],\n",
    "            ['Right Subarticular Stenosis', 'L2/L3'],\n",
    "            ['Right Subarticular Stenosis', 'L3/L4'],\n",
    "            ['Right Subarticular Stenosis', 'L4/L5'],\n",
    "            ['Right Subarticular Stenosis', 'L5/S1'],\n",
    "        ],\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = CustomConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpOm4ClDTQuy"
   },
   "source": [
    "## seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koQGTWpzTR1R"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq-XQQASm2_j"
   },
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMX5Q2_Cm2_q"
   },
   "outputs": [],
   "source": [
    "def preprocess(args):\n",
    "    train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    kf = KFold(n_splits = args.n_fold, shuffle = True, random_state = args.seed)\n",
    "    for train_indices, test_indices in kf.split(train):\n",
    "\n",
    "        train_df = train.loc[train_indices].reset_index(drop = True)\n",
    "        test_df = train.loc[test_indices].reset_index(drop = True)\n",
    "\n",
    "        folds.append([train_df, test_df])\n",
    "\n",
    "    stage1_results = [\n",
    "        np.concatenate([\n",
    "            np.load(f'{DATA_DIR}/stage1/coordinates/sgittal_t2_fold{fold+1}_preds2.npy')\n",
    "                        for fold in range(5)\n",
    "        ], axis = 0),\n",
    "\n",
    "        np.concatenate([\n",
    "            np.load(f'{DATA_DIR}/stage1/coordinates/sgittal_t1_fold{fold+1}_preds2.npy')\n",
    "                        for fold in range(5)\n",
    "        ], axis = 0),\n",
    "\n",
    "        np.concatenate([\n",
    "            np.load(f'{DATA_DIR}/stage1/coordinates/axial_t2_fold{fold+1}_preds2.npy')\n",
    "                        for fold in range(5)\n",
    "        ], axis = 0)\n",
    "    ]\n",
    "\n",
    "    df = pd.concat([\n",
    "        folds[fold][1] for fold in range(5)\n",
    "    ], axis = 0)\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    detects = {}\n",
    "    for i in range(len(df)):\n",
    "        row = df.loc[i]\n",
    "        detects[row.study_id] = {}\n",
    "        detects[row.study_id][args.scan_orientations[0]] = stage1_results[0][i]\n",
    "        detects[row.study_id][args.scan_orientations[1]] = stage1_results[1][i]\n",
    "        detects[row.study_id][args.scan_orientations[2]] = stage1_results[2][i]\n",
    "\n",
    "    return train, folds, detects\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train, folds, detects = preprocess(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxydnLNBqgjZ"
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O9uUuhBqgja"
   },
   "outputs": [],
   "source": [
    "class CustomTransform(nn.Module):\n",
    "    def __init__(self, args, mode = 'test'):\n",
    "        super(CustomTransform, self).__init__()\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(args.image_size, args.image_size, p = 1.0),\n",
    "                A.HorizontalFlip(p = 0.5),\n",
    "                A.VerticalFlip(p = 0.5),\n",
    "                A.ShiftScaleRotate(border_mode = cv2.BORDER_CONSTANT, p = 0.75),\n",
    "                A.RandomBrightnessContrast(p = 0.75),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(args.image_size, args.image_size, p = 1.0),\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2, 0)\n",
    "        x = self.transform(image = x)['image']\n",
    "        x = x.transpose(2, 0, 1)\n",
    "        x = torch.tensor(x, dtype = torch.float)\n",
    "        return x\n",
    "\n",
    "def get_global_weights(args, df):\n",
    "    weights = np.zeros([len(df), len(args.label_columns)], dtype = np.float32)\n",
    "    for i in range(weights.shape[0]):\n",
    "        row = df.loc[i]\n",
    "\n",
    "        label = row[args.label_columns]\n",
    "        label = label.fillna('Nan')\n",
    "        label = label.values\n",
    "\n",
    "        for j in range(weights.shape[1]):\n",
    "            weights[i, j] = args.label2weight[label[j]]\n",
    "\n",
    "    spinal_weight = weights[:, 0:5]\n",
    "    spinal_weight = spinal_weight / spinal_weight.sum()\n",
    "    spinal_weight = spinal_weight * spinal_weight.size\n",
    "\n",
    "    foraminal_weight = weights[:, 5:15]\n",
    "    foraminal_weight = foraminal_weight / foraminal_weight.sum()\n",
    "    foraminal_weight = foraminal_weight * foraminal_weight.size\n",
    "\n",
    "    subarticular_weight = weights[:, 15:25]\n",
    "    subarticular_weight = subarticular_weight / subarticular_weight.sum()\n",
    "    subarticular_weight = subarticular_weight * subarticular_weight.size\n",
    "\n",
    "    any_severe_spinal_weight = weights[:, 0:5].max(axis = 1, keepdims = True)\n",
    "    any_severe_spinal_weight = any_severe_spinal_weight / any_severe_spinal_weight.sum()\n",
    "    any_severe_spinal_weight = any_severe_spinal_weight * any_severe_spinal_weight.size\n",
    "\n",
    "    weights = np.concatenate([spinal_weight, foraminal_weight, subarticular_weight, any_severe_spinal_weight], axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "eIi1-Fr9qgjb",
    "outputId": "22fb7b95-5fc0-40d5-aff6-96996c57f82c"
   },
   "outputs": [],
   "source": [
    "def convert_to_8bit(x):\n",
    "    lower, upper = np.percentile(x, (1, 99))\n",
    "    x = np.clip(x, lower, upper)\n",
    "    x = x - np.min(x)\n",
    "    x = x / np.max(x)\n",
    "    return (x * 255).astype(\"uint8\")\n",
    "\n",
    "def get_imgs(dcms):\n",
    "    imgs = []\n",
    "    for dcm in dcms:\n",
    "        img = convert_to_8bit(dcm.pixel_array)\n",
    "        imgs.append(img)\n",
    "\n",
    "    try:\n",
    "        return np.stack(imgs, axis = 0)\n",
    "    except:\n",
    "        h, w = imgs[0].shape\n",
    "        imgs = [A.Resize(h, w)(image = x)['image'] for x in imgs]\n",
    "        return np.stack(imgs, axis = 0)\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 df,\n",
    "                 detects,\n",
    "                 mode = 'test',\n",
    "                 ):\n",
    "        self.args = args\n",
    "        self.df = df\n",
    "        self.detects = detects\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.transform = CustomTransform(args, self.mode)\n",
    "\n",
    "        self.volume_sizes = {\n",
    "            'Sagittal T2/STIR' : [29, 512, 512],\n",
    "            'Sagittal T1' : [38, 512, 512],\n",
    "            'Axial T2' : [192, 512, 512]\n",
    "        }\n",
    "\n",
    "        self.train_series_descriptions = pd.read_csv(f'{DATA_DIR}/train_series_descriptions.csv')\n",
    "        self.train_label_coordinates = pd.read_csv(f'{DATA_DIR}/train_label_coordinates.csv')\n",
    "\n",
    "        self.weights = get_global_weights(args, df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "    def get_subvolume(self, study_id, series_id, orientation):\n",
    "        dicoms = sorted(glob.glob(self.args.root + f'/train_images/{study_id}/{series_id}/*.dcm'), key = lambda x: int(x.split('/')[-1].split('.')[0]))     \n",
    "        dicoms = [pydicom.dcmread(x) for x in dicoms]\n",
    "        \n",
    "        if 'Sagittal' in orientation:\n",
    "            pos = np.asarray([dicom.ImagePositionPatient for dicom in dicoms])[:, 0]\n",
    "            orien = np.asarray([dicom.ImageOrientationPatient for dicom in dicoms])[:, 0]\n",
    "        else:\n",
    "            pos = np.asarray([dicom.ImagePositionPatient for dicom in dicoms])[:, -1]\n",
    "            orien = np.asarray([dicom.ImageOrientationPatient for dicom in dicoms])[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "        inputs = get_imgs(dicoms)\n",
    "        inputs = A.Resize(self.volume_sizes[orientation][1], self.volume_sizes[orientation][2])(image = inputs.transpose(1, 2, 0))['image'].transpose(2, 0, 1)\n",
    "        inputs = torch.tensor(inputs, dtype = torch.float)\n",
    "        inputs = inputs / 255.0\n",
    "\n",
    "        return inputs, pos, orien\n",
    "\n",
    "    def get_volume(self, row, orientation):\n",
    "        row_series_descriptions = self.train_series_descriptions[self.train_series_descriptions['study_id'] == row['study_id']]\n",
    "        row_series_descriptions = row_series_descriptions[row_series_descriptions['series_description'] == orientation].reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        if len(row_series_descriptions) > 0:\n",
    "            inputs, pos, orien = [], [], []\n",
    "            for i in range(len(row_series_descriptions)):\n",
    "                study_id, series_id, _ = row_series_descriptions.loc[i]\n",
    "                \n",
    "                _inputs, _pos, _orien = self.get_subvolume(study_id, series_id, orientation)\n",
    "                inputs.append(_inputs)\n",
    "                pos.append(_pos)\n",
    "                orien.append(_orien)\n",
    "\n",
    "            inputs = torch.cat(inputs, dim = 0)\n",
    "            pos = np.concatenate(pos, axis = 0)\n",
    "            orien = np.concatenate(orien, axis = 0)\n",
    "        else:\n",
    "            inputs = torch.zeros(self.volume_sizes[orientation], dtype = torch.float32)\n",
    "            pos = np.zeros([self.volume_sizes[orientation][0]], dtype = np.float32)\n",
    "            orien = np.zeros([self.volume_sizes[orientation][0]], dtype = np.float32)\n",
    "\n",
    "        pos = np.argsort(pos)\n",
    "        inputs = inputs[pos]\n",
    "        orien = orien[pos]\n",
    "\n",
    "        if inputs.shape[0] > self.volume_sizes[orientation][0]:\n",
    "            inputs = F.interpolate(inputs.unsqueeze(0).unsqueeze(0), size = self.volume_sizes[orientation], mode = 'trilinear').squeeze(0).squeeze(0)\n",
    "            orien = zoom(orien, self.volume_sizes[orientation][0]/orien.shape[0])\n",
    "        return inputs, orien\n",
    "\n",
    "    def get_inputs(self, row):\n",
    "        row_series_descriptions = self.train_series_descriptions[self.train_series_descriptions['study_id'] == row['study_id']]\n",
    "\n",
    "        inputs = []\n",
    "        for orientation in args.scan_orientations:\n",
    "            if orientation in list(row_series_descriptions['series_description']):\n",
    "                volume, orien = self.get_volume(row, orientation)\n",
    "                detect = self.detects[row['study_id']][orientation]\n",
    "\n",
    "\n",
    "                crops = []\n",
    "                for i in range(detect.shape[0]):\n",
    "                    z = detect[i][0] * 1\n",
    "                    z_all = range(z-2, z+3)\n",
    "                    y_all = detect[i][1:10] * 2\n",
    "                    x_all = detect[i][10:20] * 2\n",
    "\n",
    "                    y_all, x_all = y_all[2:-2], x_all[2:-2]\n",
    "\n",
    "                    crop = []\n",
    "                    for z, y, x in zip(z_all, y_all, x_all):\n",
    "\n",
    "                        z = min(max(z, 0), volume.shape[0]-1)\n",
    "                        y_start, y_end = y - self.args.patch_size, y + self.args.patch_size\n",
    "                        if y_start<0:\n",
    "                            y_start, y_end = 0, 2*self.args.patch_size\n",
    "                        if y_end>=self.volume_sizes[orientation][1]:\n",
    "                            y_start, y_end = self.volume_sizes[orientation][1]-1 - 2*self.args.patch_size, self.volume_sizes[orientation][1]-1\n",
    "\n",
    "                        x_start, x_end = x - self.args.patch_size, x + self.args.patch_size\n",
    "                        if x_start<0:\n",
    "                            x_start, x_end = 0, 2*self.args.patch_size\n",
    "                        if x_end>=self.volume_sizes[orientation][2]:\n",
    "                            x_start, x_end = self.volume_sizes[orientation][2]-1 - 2*self.args.patch_size, self.volume_sizes[orientation][2]-1\n",
    "\n",
    "                        crop.append(volume[z, y_start:y_end, x_start:x_end])\n",
    "                \n",
    "                    crop = torch.stack(crop, dim = 0)\n",
    "\n",
    "                    \n",
    "                    crop = crop.numpy()\n",
    "\n",
    "                    crops.append(crop)\n",
    "                crops = np.concatenate(crops, axis = 0)\n",
    "            else:\n",
    "                crops = np.zeros([self.args.depth_size * len(self.args.coordinate_columns[orientation]), 2*self.args.patch_size, 2*self.args.patch_size])\n",
    "\n",
    "            crops = crops.astype(np.float32)\n",
    "            crops = self.transform(crops)\n",
    "            crops = crops.reshape(-1, self.args.depth_size, self.args.image_size, self.args.image_size)\n",
    "            inputs.append(crops)\n",
    "\n",
    "        inputs = np.concatenate(inputs, axis = 0)\n",
    "        inputs = torch.tensor(inputs, dtype = torch.float)\n",
    "        return inputs\n",
    "\n",
    "    def get_targets(self, row):\n",
    "        label = row[self.args.label_columns]\n",
    "        label = label.fillna('Nan')\n",
    "\n",
    "        targets = [self.args.label2index[x] for x in label]\n",
    "        targets = torch.tensor(targets, dtype = torch.long)\n",
    "        return targets\n",
    "\n",
    "    def get_weights(self, index):\n",
    "        weights = self.weights[index]\n",
    "        weights = torch.tensor(weights, dtype = torch.float)\n",
    "        return weights\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.loc[index]\n",
    "\n",
    "        inputs = self.get_inputs(row)\n",
    "        targets = self.get_targets(row)\n",
    "        weights = self.get_weights(index)\n",
    "        \n",
    "        return inputs, targets, weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_df, test_df = folds[0]\n",
    "\n",
    "    mode = 'train'\n",
    "\n",
    "    df = train_df if mode == 'train' else test_df\n",
    "    dataset = CustomDataset(args, df, detects, mode)\n",
    "\n",
    "    index = random.randint(0, len(dataset) - 1)\n",
    "    inputs, targets, weights = dataset[index]\n",
    "\n",
    "    print('inputs : ', inputs.shape)\n",
    "    print('targets : ', targets)\n",
    "    print('weights : ', weights)\n",
    "\n",
    "    index = random.randint(0, inputs.shape[0] - 1)\n",
    "    volume = inputs[index]\n",
    "    fig, axes = plt.subplots(1, volume.shape[0], figsize = (5 * volume.shape[0], 5))\n",
    "    for i in range(volume.shape[0]):\n",
    "        x = volume[i]\n",
    "        axes[i].imshow(x, cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "    inputs1, inputs2, inputs3 = inputs[0:5], inputs[5:15], inputs[15:25]\n",
    "\n",
    "    print('Sagittal T2/STIR : ', inputs1.shape)\n",
    "    print('Sagittal T1 : ', inputs2.shape)\n",
    "    print('Axial T2 : ', inputs3.shape)\n",
    "\n",
    "    for inputs in [inputs1, inputs2, inputs3]:\n",
    "        fig, axes = plt.subplots(1, inputs.shape[0], figsize = (5 * inputs.shape[0], 5))\n",
    "        for i in range(inputs.shape[0]):\n",
    "            x = inputs[i]\n",
    "            axes[i].imshow(x[x.shape[0]//2], cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af5XU7rpWXd9"
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, n_channel, hidden_size):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            # model_name = 'convnext_tiny.in12k_ft_in1k',\n",
    "            model_name = 'convnext_small.in12k_ft_in1k',\n",
    "            pretrained = True,\n",
    "            num_classes = 0,\n",
    "            in_chans = n_channel\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class CustomPooler(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CustomPooler, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size, bias = True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias = False),\n",
    "            nn.Softmax(dim = 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _x = self.fc(x)\n",
    "        x = torch.sum(x * _x, dim = 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 n_channel = 1,\n",
    "                 hidden_size = 768,\n",
    "                 drop_rate = 0.2\n",
    "                 ):\n",
    "\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.args = args\n",
    "        self.version = args.version\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.cnn = FeatureExtractor(\n",
    "            n_channel = n_channel,\n",
    "            hidden_size = hidden_size,\n",
    "            )\n",
    "\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size = hidden_size,\n",
    "            hidden_size = hidden_size//2,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "            )\n",
    "\n",
    "        self.pooler = CustomPooler(\n",
    "            hidden_size = hidden_size,\n",
    "            )\n",
    "        \n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size = hidden_size,\n",
    "            hidden_size = hidden_size//2,\n",
    "            batch_first = True,\n",
    "            bidirectional = True,\n",
    "            )\n",
    "\n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Dropout(2 * drop_rate),\n",
    "            nn.Linear(hidden_size, args.n_class)\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hidden_size, args.n_class)\n",
    "        )\n",
    "        self.out3 = nn.Sequential(\n",
    "            nn.Dropout(2 * drop_rate),\n",
    "            nn.Linear(hidden_size, args.n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, d, h, w = x.shape\n",
    "\n",
    "        x = x.reshape(-1, h, w)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "\n",
    "        if self.version == 1:\n",
    "            x = x.reshape(-1, d, self.hidden_size)\n",
    "            x, _ = self.rnn1(x)\n",
    "            x = self.pooler(x)\n",
    "            x = x.reshape(-1, 25, self.hidden_size)\n",
    "            x = x.reshape(-1, 5, 5, self.hidden_size)\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "            x = x.reshape(-1, 5, self.hidden_size)\n",
    "            x, _ = self.rnn2(x)\n",
    "\n",
    "        elif self.version == 2:\n",
    "            x = x.reshape(-1, d, self.hidden_size)\n",
    "            x, _ = self.rnn1(x)\n",
    "            x = self.pooler(x)\n",
    "            x = x.reshape(-1, 25, self.hidden_size)\n",
    "            x = x.reshape(-1, 5, 5, self.hidden_size)\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "            x = x.reshape(-1, 5, self.hidden_size)\n",
    "            x_, _ = self.rnn2(x)\n",
    "            x = x + x_\n",
    "\n",
    "        elif self.version == 3:\n",
    "            x = x.reshape(-1, 5, 5, d, self.hidden_size)\n",
    "            x = x.permute(0, 2, 1, 3, 4)\n",
    "            x = x.reshape(-1, 5 * d, self.hidden_size)\n",
    "            x, _ = self.rnn1(x)\n",
    "            x = x.reshape(-1, d, self.hidden_size)\n",
    "            x = self.pooler(x)\n",
    "        \n",
    "        x = x.reshape(-1, 5, 5, self.hidden_size)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(-1, 25, self.hidden_size)\n",
    "                \n",
    "\n",
    "        x1 = x[:, 0:5]\n",
    "        x2 = x[:, 5:15]\n",
    "        x3 = x[:, 15:]\n",
    "\n",
    "        x1 = self.out1(x1)\n",
    "        x2 = self.out2(x2)\n",
    "        x3 = self.out3(x3)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim = 1)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
    "    sample = next(iter(loader))\n",
    "    sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "    model = CustomModel(args)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sample[0])\n",
    "        print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9NzV_ZYP9fr"
   },
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suswnABJP9fs",
    "outputId": "55694fe7-70db-48ee-8c62-121b542b59e7"
   },
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        self.bce_loss = nn.BCELoss(reduction = 'none')\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.severe_index = args.label2index['Severe']\n",
    "\n",
    "    def forward(self, preds, trues, weights):\n",
    "        if not self.args.weight_loss:\n",
    "            weights = (weights!=0).float()\n",
    "\n",
    "        spinal_pred = preds[:, 0:5, :].reshape(-1, self.args.n_class)\n",
    "        spinal_true = trues[:, 0:5].reshape(-1)\n",
    "        spinal_weight = weights[:, 0:5].reshape(-1)\n",
    "        spinal_loss = self.ce_loss(spinal_pred, spinal_true)\n",
    "\n",
    "        foraminal_pred = preds[:, 5:15, :].reshape(-1, self.args.n_class)\n",
    "        foraminal_true = trues[:, 5:15].reshape(-1)\n",
    "        foraminal_weight = weights[:, 5:15].reshape(-1)\n",
    "        foraminal_loss = self.ce_loss(foraminal_pred, foraminal_true)\n",
    "\n",
    "        subarticular_pred = preds[:, 15:25, :].reshape(-1, self.args.n_class)\n",
    "        subarticular_true = trues[:, 15:25].reshape(-1)\n",
    "        subarticular_weight = weights[:, 15:25].reshape(-1)\n",
    "        subarticular_loss = self.ce_loss(subarticular_pred, subarticular_true)\n",
    "\n",
    "        any_severe_spinal_true = ((trues[:, 0:5] == self.severe_index).sum(1) != 0).float()\n",
    "        any_severe_spinal_pred, _ = self.softmax(preds)[:, 0:5, self.severe_index].max(1)\n",
    "        any_severe_spinal_weight = weights[:, 25]\n",
    "        any_severe_spinal_loss = self.bce_loss(any_severe_spinal_pred, any_severe_spinal_true)\n",
    "\n",
    "        spinal_loss = (spinal_loss * spinal_weight).mean()\n",
    "        foraminal_loss = (foraminal_loss * foraminal_weight).mean()\n",
    "        subarticular_loss = (subarticular_loss * subarticular_weight).mean()\n",
    "        any_severe_spinal_loss = (any_severe_spinal_loss * any_severe_spinal_weight).mean()\n",
    "\n",
    "        if self.args.any_loss:\n",
    "            loss = (spinal_loss + foraminal_loss + subarticular_loss + any_severe_spinal_loss) / 4\n",
    "        else:\n",
    "            loss = (spinal_loss + foraminal_loss + subarticular_loss) / 3\n",
    "        return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
    "    sample = next(iter(loader))\n",
    "    sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "    model = CustomModel(args)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    loss_fn = CustomLoss(args)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sample[0])\n",
    "        loss = loss_fn(outputs, sample[1], sample[2])\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Sj9j4J_K1Dk"
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF5f7GiLK2BM"
   },
   "outputs": [],
   "source": [
    "def get_solution(args, df):\n",
    "    row_id = []\n",
    "    normal_mild = []\n",
    "    moderate = []\n",
    "    severe = []\n",
    "    sample_weight = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x = df.loc[i]\n",
    "        x = x.fillna('Nan')\n",
    "\n",
    "        for j in range(len(args.label_columns)):\n",
    "            label = x[args.label_columns[j]]\n",
    "            onehot = args.label2onehot[label]\n",
    "            weight = args.label2weight[label]\n",
    "\n",
    "            row_id.append(f'{x.study_id}_{args.label_columns[j]}')\n",
    "            normal_mild.append(onehot[0])\n",
    "            moderate.append(onehot[1])\n",
    "            severe.append(onehot[2])\n",
    "            sample_weight.append(weight)\n",
    "\n",
    "    solution = pd.DataFrame()\n",
    "\n",
    "    solution['row_id'] = row_id\n",
    "    solution['normal_mild'] = normal_mild\n",
    "    solution['moderate'] = moderate\n",
    "    solution['severe'] = severe\n",
    "    solution['sample_weight'] = sample_weight\n",
    "    return solution\n",
    "\n",
    "def get_submission(args, df, preds):\n",
    "    assert preds.shape == (len(df), len(args.label_columns), 3)\n",
    "\n",
    "    row_id = []\n",
    "    normal_mild = []\n",
    "    moderate = []\n",
    "    severe = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x = df.loc[i]\n",
    "        x = x.fillna('Nan')\n",
    "\n",
    "        for j in range(len(args.label_columns)):\n",
    "            pred = preds[i, j, :]\n",
    "\n",
    "            row_id.append(f'{x.study_id}_{args.label_columns[j]}')\n",
    "            normal_mild.append(pred[0])\n",
    "            moderate.append(pred[1])\n",
    "            severe.append(pred[2])\n",
    "\n",
    "    submission = pd.DataFrame()\n",
    "\n",
    "    submission['row_id'] = row_id\n",
    "    submission['normal_mild'] = normal_mild\n",
    "    submission['moderate'] = moderate\n",
    "    submission['severe'] = severe\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQY6_UNwK2EW"
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/code/metric/rsna-lumbar-metric-71549\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_condition(full_location: str) -> str:\n",
    "    # Given an input like spinal_canal_stenosis_l1_l2 extracts 'spinal'\n",
    "    for injury_condition in ['spinal', 'foraminal', 'subarticular']:\n",
    "        if injury_condition in full_location:\n",
    "            return injury_condition\n",
    "    raise ValueError(f'condition not found in {full_location}')\n",
    "\n",
    "\n",
    "def score_function(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        row_id_column_name: str = 'row_id',\n",
    "        any_severe_scalar: float = 1.0,\n",
    "    ) -> float:\n",
    "    '''\n",
    "    Pseudocode:\n",
    "    1. Calculate the sample weighted log loss for each medical condition:\n",
    "    2. Derive a new any_severe label.\n",
    "    3. Calculate the sample weighted log loss for the new any_severe label.\n",
    "    4. Return the average of all of the label group log losses as the final score, normalized for the number of columns in each group.\n",
    "       This mitigates the impact of spinal stenosis having only half as many columns as the other two conditions.\n",
    "    '''\n",
    "\n",
    "    target_levels = ['normal_mild', 'moderate', 'severe']\n",
    "\n",
    "    # Run basic QC checks on the inputs\n",
    "    if not pandas.api.types.is_numeric_dtype(submission[target_levels].values):\n",
    "        raise ParticipantVisibleError('All submission values must be numeric')\n",
    "\n",
    "    if not np.isfinite(submission[target_levels].values).all():\n",
    "        raise ParticipantVisibleError('All submission values must be finite')\n",
    "\n",
    "    if solution[target_levels].min().min() < 0:\n",
    "        raise ParticipantVisibleError('All labels must be at least zero')\n",
    "    if submission[target_levels].min().min() < 0:\n",
    "        raise ParticipantVisibleError('All predictions must be at least zero')\n",
    "\n",
    "    solution['study_id'] = solution['row_id'].apply(lambda x: x.split('_')[0])\n",
    "    solution['location'] = solution['row_id'].apply(lambda x: '_'.join(x.split('_')[1:]))\n",
    "    solution['condition'] = solution['row_id'].apply(get_condition)\n",
    "\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    assert sorted(submission.columns) == sorted(target_levels)\n",
    "\n",
    "    submission['study_id'] = solution['study_id']\n",
    "    submission['location'] = solution['location']\n",
    "    submission['condition'] = solution['condition']\n",
    "\n",
    "    condition_losses = []\n",
    "    condition_weights = []\n",
    "    for condition in ['spinal', 'foraminal', 'subarticular']:\n",
    "        condition_indices = solution.loc[solution['condition'] == condition].index.values\n",
    "        condition_loss = sklearn.metrics.log_loss(\n",
    "            y_true=solution.loc[condition_indices, target_levels].values,\n",
    "            y_pred=submission.loc[condition_indices, target_levels].values,\n",
    "            sample_weight=solution.loc[condition_indices, 'sample_weight'].values\n",
    "        )\n",
    "        condition_losses.append(condition_loss)\n",
    "        condition_weights.append(1)\n",
    "\n",
    "    any_severe_spinal_labels = pd.Series(solution.loc[solution['condition'] == 'spinal'].groupby('study_id')['severe'].max())\n",
    "    any_severe_spinal_weights = pd.Series(solution.loc[solution['condition'] == 'spinal'].groupby('study_id')['sample_weight'].max())\n",
    "    any_severe_spinal_predictions = pd.Series(submission.loc[submission['condition'] == 'spinal'].groupby('study_id')['severe'].max())\n",
    "    any_severe_spinal_loss = sklearn.metrics.log_loss(\n",
    "        y_true=any_severe_spinal_labels,\n",
    "        y_pred=any_severe_spinal_predictions,\n",
    "        sample_weight=any_severe_spinal_weights\n",
    "    )\n",
    "    condition_losses.append(any_severe_spinal_loss)\n",
    "    condition_weights.append(any_severe_scalar)\n",
    "    results = {\n",
    "        'score' : np.average(condition_losses, weights=condition_weights),\n",
    "        'spinal-score' : condition_losses[0],\n",
    "        'foraminal-score' : condition_losses[1],\n",
    "        'subarticular-score' : condition_losses[2],\n",
    "        'any-severe-spinal-score' : condition_losses[3],\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCqQT_2jVSQX"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnyL8uZAVSQZ"
   },
   "outputs": [],
   "source": [
    "# ref.: https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    H = size[3]\n",
    "    W = size[4]\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cut_w = int(W * cut_rat)\n",
    "\n",
    "    cy = np.random.randint(H)\n",
    "    cx = np.random.randint(W)\n",
    "\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "\n",
    "    return bby1, bby2, bbx1, bbx2\n",
    "\n",
    "def cutmix(inputs, targets, weights, alpha):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "\n",
    "    bby1, bby2, bbx1, bbx2 = rand_bbox(inputs.size(), lam)\n",
    "    inputs[:, :, :, bby1:bby2, bbx1:bbx2] = inputs[rand_index, :, :, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "    lam = 1 - ((bby2 - bby1) * (bbx2 - bbx1) / (inputs.size()[-2] * inputs.size()[-1]))\n",
    "\n",
    "    target_a = targets\n",
    "    target_b = targets[rand_index]\n",
    "\n",
    "    weight_a = weights\n",
    "    weight_b = weights[rand_index]\n",
    "    return inputs, (target_a, target_b), (weight_a, weight_b), lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG0FGW4rVSQa"
   },
   "outputs": [],
   "source": [
    "# ref.: https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
    "\n",
    "def mixup(inputs, targets, weights, alpha):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "\n",
    "    inputs = lam * inputs + (1 - lam) * inputs[rand_index, :]\n",
    "\n",
    "    target_a = targets\n",
    "    target_b = targets[rand_index]\n",
    "\n",
    "    weight_a = weights\n",
    "    weight_b = weights[rand_index]\n",
    "    return inputs, (target_a, target_b), (weight_a, weight_b), lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWiG-Ik8VSQa"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer:\n",
    "    def __init__(self, args, model):\n",
    "        self.model = model\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "\n",
    "        self.log_path = f'{args.save_dir}/log.txt'\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr = args.lr, weight_decay = args.wd)\n",
    "\n",
    "        total_steps = args.total_steps\n",
    "        warmup_steps = int(total_steps * args.warmup_ratio)\n",
    "        print('total_steps: ', total_steps)\n",
    "        print('warmup_steps: ', warmup_steps)\n",
    "\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(self.optimizer,\n",
    "                                                         num_warmup_steps = warmup_steps,\n",
    "                                                         num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "        self.train_loss_fn = CustomLoss(args)\n",
    "        self.test_loss_fn = CustomLoss(args)\n",
    "\n",
    "    def run(self, args, train_loader, test_loader):\n",
    "        bst_auc = 0\n",
    "        for epoch in range(args.n_epoch):\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            train_loss = self.train_function(args, train_loader)\n",
    "\n",
    "            train_log = f'epoch : {epoch + 1}, lr : {lr}, train_loss: {train_loss:.6f}'\n",
    "            self.log(args, train_log)\n",
    "\n",
    "            if ((epoch + 1) % args.test_freq) == 0:\n",
    "                test_loss, test_score, test_results = self.test_function(args, test_loader)\n",
    "\n",
    "                test_log = f'epoch : {epoch + 1}, lr : {lr}, test_loss: {test_loss:.6f}, test_score: {test_score:.6f}\\n'\n",
    "                self.log(args, test_log)\n",
    "\n",
    "                score_log = json.dumps(test_results, indent = 4)\n",
    "                self.log(args, score_log)\n",
    "\n",
    "                if test_results['auc'] < bst_auc:\n",
    "                    !rm {args.save_dir}/epoch*\n",
    "                    bst_auc =  test_results['auc']\n",
    "                    save_path = args.save_dir + '/epoch' + f'{epoch + 1}'.zfill(3) + \\\n",
    "                                f'-trainloss{round(train_loss, 6)}' + \\\n",
    "                                f'-testloss{round(test_loss, 6)}' + \\\n",
    "                                f'-testscore{round(test_score, 6)}' + '.bin'\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    def train_function(self, args, loader):\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for bi, sample in enumerate(tqdm(loader)):\n",
    "            sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "            inputs = sample[0]\n",
    "            targets = sample[1]\n",
    "            weights = sample[2]\n",
    "\n",
    "            mix = (np.random.rand() < args.mix_prob)\n",
    "\n",
    "            if mix:\n",
    "                if args.mix_method == 'cutmix':\n",
    "                    inputs, targets, weights, lam = cutmix(inputs, targets, weights, args.mix_alpha)\n",
    "                elif args.mix_method == 'mixup':\n",
    "                    inputs, targets, weights, lam = mixup(inputs, targets, weights, args.mix_alpha)\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "\n",
    "            with torch.amp.autocast(args.device):\n",
    "                outputs = self.model(inputs)\n",
    "                outputs = outputs.float()\n",
    "\n",
    "            if mix:\n",
    "                loss = self.train_loss_fn(outputs, targets[0], weights[0]) * lam + self.train_loss_fn(outputs, targets[1], weights[1]) * (1 - lam)\n",
    "            else:\n",
    "                loss = self.train_loss_fn(outputs, targets, weights)\n",
    "\n",
    "\n",
    "            loss = loss / args.iters_to_accumulate\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if (bi + 1) % args.iters_to_accumulate == 0:\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                self.scheduler.step()\n",
    "\n",
    "            total_loss += loss.detach().cpu().tolist() * args.iters_to_accumulate\n",
    "\n",
    "        return total_loss/len(loader)\n",
    "\n",
    "    def test_function(self, args, loader):\n",
    "        self.model.eval()\n",
    "\n",
    "        preds = torch.zeros((len(loader.dataset), len(args.label_columns), 3), dtype = torch.float)\n",
    "        trues = torch.zeros((len(loader.dataset), len(args.label_columns)), dtype = torch.long)\n",
    "        masks = torch.zeros((len(loader.dataset), len(args.label_columns)+1), dtype = torch.float)\n",
    "        for bi, sample in enumerate(tqdm(loader)):\n",
    "            sample = [x.to(args.device) for x in sample]\n",
    "\n",
    "            inputs = sample[0]\n",
    "            targets = sample[1]\n",
    "            weights = sample[2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "            preds[args.batch_size * bi:args.batch_size * (bi + 1)] = outputs.detach().cpu()\n",
    "            trues[args.batch_size * bi:args.batch_size * (bi + 1)] = targets.detach().cpu()\n",
    "            masks[args.batch_size * bi:args.batch_size * (bi + 1)] = weights.detach().cpu()\n",
    "\n",
    "        total_loss = self.test_loss_fn(preds, trues, masks)\n",
    "\n",
    "        official_results = self.get_official_score(args, preds, trues, loader)\n",
    "        internal_results = self.get_internal_score(args, preds, trues)\n",
    "\n",
    "        results = (official_results | internal_results)\n",
    "        return total_loss.detach().cpu().tolist(), results['score'], results\n",
    "\n",
    "    def get_official_score(self, args, preds, trues, loader):\n",
    "        solution = get_solution(args, loader.dataset.df)\n",
    "\n",
    "        preds = nn.Softmax(dim = -1)(preds)\n",
    "        preds = preds.numpy()\n",
    "        submission = get_submission(args, loader.dataset.df, preds)\n",
    "\n",
    "        nonzero_indices = (solution['sample_weight'] > 0)\n",
    "        solution = solution[nonzero_indices].reset_index(drop = True)\n",
    "        submission = submission[nonzero_indices].reset_index(drop = True)\n",
    "\n",
    "        score = score_function(solution = solution.copy(), submission = submission.copy())\n",
    "        return score\n",
    "\n",
    "    def get_internal_score(self, args, preds, trues):\n",
    "        preds = nn.Softmax(dim = -1)(preds)\n",
    "\n",
    "        spinal_preds = preds[:, 0:5].reshape(-1, 3)\n",
    "        foraminal_preds = preds[:, 5:15].reshape(-1, 3)\n",
    "        subarticular_preds = preds[:, 15:25].reshape(-1, 3)\n",
    "        any_severe_spinal_preds = preds[:, 0:5, args.label2index['Severe']].max(1).values\n",
    "\n",
    "        spinal_trues = trues[:, 0:5].reshape(-1)\n",
    "        foraminal_trues = trues[:, 5:15].reshape(-1)\n",
    "        subarticular_trues = trues[:, 15:25].reshape(-1)\n",
    "        any_severe_spinal_trues = ((trues[:, 0:5] == args.label2index['Severe']).sum(1) != 0).long()\n",
    "\n",
    "        spinal_auc = roc_auc_score(\n",
    "            spinal_trues[spinal_trues>=0],\n",
    "            spinal_preds[spinal_trues>=0],\n",
    "            multi_class = 'ovr'\n",
    "            )\n",
    "\n",
    "        foraminal_auc = roc_auc_score(\n",
    "            foraminal_trues[foraminal_trues>=0],\n",
    "            foraminal_preds[foraminal_trues>=0],\n",
    "            multi_class = 'ovr'\n",
    "            )\n",
    "\n",
    "        subarticular_auc = roc_auc_score(\n",
    "            subarticular_trues[subarticular_trues>=0],\n",
    "            subarticular_preds[subarticular_trues>=0],\n",
    "            multi_class = 'ovr'\n",
    "            )\n",
    "\n",
    "        any_severe_spinal_auc = roc_auc_score(\n",
    "            any_severe_spinal_trues[any_severe_spinal_trues>=0],\n",
    "            any_severe_spinal_preds[any_severe_spinal_trues>=0],\n",
    "            )\n",
    "\n",
    "        results = {\n",
    "            'auc' : np.mean([spinal_auc, foraminal_auc, subarticular_auc, any_severe_spinal_auc]),\n",
    "            'spinal-auc' : spinal_auc,\n",
    "            'foraminal-auc' : foraminal_auc,\n",
    "            'subarticular-auc' : subarticular_auc,\n",
    "            'any-severe-spinal-auc' : any_severe_spinal_auc,\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def log(self, args, message):\n",
    "        print(message)\n",
    "        with open(f'{args.save_dir}/log.txt', 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDusspehrlvY"
   },
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFfUUgM2rlvZ",
    "outputId": "bd43fe18-a8f6-476b-92f7-073b067f7527"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for SEED in [42, 2024]:\n",
    "        for VERSION in [1, 2, 3]:\n",
    "            \n",
    "            CustomConfig.seed = SEED\n",
    "            CustomConfig.version = VERSION\n",
    "            args = CustomConfig()\n",
    "\n",
    "            train, folds, detects = preprocess(args)\n",
    "            for i in range(args.n_fold):\n",
    "                seed_everything(args.seed)\n",
    "\n",
    "                train_df, test_df = folds[i]\n",
    "\n",
    "                train_dataset = CustomDataset(args, train_df, detects, mode = 'train')\n",
    "                test_dataset = CustomDataset(args, test_df, detects, mode = 'test')\n",
    "\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size = args.batch_size,\n",
    "                                                           num_workers = args.n_worker,\n",
    "                                                           shuffle = True,\n",
    "                                                           drop_last = True)\n",
    "                test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                          batch_size = args.batch_size,\n",
    "                                                          num_workers = args.n_worker,\n",
    "                                                          shuffle = False,\n",
    "                                                          drop_last = False)\n",
    "\n",
    "                model = CustomModel(args)\n",
    "                model = model.to(args.device)\n",
    "\n",
    "                name = f'/stage2_{SEED}_{VERSION}'\n",
    "                args.save_dir = DATA_DIR + name + f'/fold{i+1}'\n",
    "                args.total_steps = int(len(train_df) * args.n_epoch/(args.batch_size * args.iters_to_accumulate))\n",
    "\n",
    "                trainer = CustomTrainer(args, model)\n",
    "                trainer.run(args, train_loader, test_loader)\n",
    "                del model, trainer\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomConfig.weight_loss = True\n",
    "CustomConfig.any_loss = True\n",
    "CustomConfig.n_epoch = 10\n",
    "\n",
    "# the same model : the only change is to compute grad only on the head layer\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 n_channel = 1,\n",
    "                 hidden_size = 768,\n",
    "                 drop_rate = 0.2\n",
    "                 ):\n",
    "\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.version = args.version\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        self.cnn = FeatureExtractor(\n",
    "            n_channel = n_channel,\n",
    "            hidden_size = hidden_size,\n",
    "            )\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size = hidden_size,\n",
    "            hidden_size = hidden_size//2,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "            )\n",
    "\n",
    "        self.pooler = CustomPooler(\n",
    "            hidden_size = hidden_size,\n",
    "            )\n",
    "\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size = hidden_size,\n",
    "            hidden_size = hidden_size//2,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "            )\n",
    "\n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(2 * hidden_size, args.n_class)\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hidden_size, args.n_class)\n",
    "        )\n",
    "        self.out3 = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(2 * hidden_size, args.n_class)\n",
    "        )\n",
    "\n",
    "    def initialize(self):\n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Dropout(self.drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.args.n_class)\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            nn.Dropout(self.drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.args.n_class)\n",
    "        )\n",
    "        self.out3 = nn.Sequential(\n",
    "            nn.Dropout(self.drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.args.n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            _, _, d, h, w = x.shape\n",
    "\n",
    "            x = x.reshape(-1, h, w)\n",
    "            x = x.unsqueeze(1)\n",
    "            x = self.cnn(x)\n",
    "\n",
    "            if self.version == 1:\n",
    "                x = x.reshape(-1, d, self.hidden_size)\n",
    "                x, _ = self.rnn1(x)\n",
    "                x = self.pooler(x)\n",
    "                x = x.reshape(-1, 25, self.hidden_size)\n",
    "                x = x.reshape(-1, 5, 5, self.hidden_size)\n",
    "                x = x.permute(0, 2, 1, 3)\n",
    "                x = x.reshape(-1, 5, self.hidden_size)\n",
    "                x, _ = self.rnn2(x)\n",
    "\n",
    "            elif self.version == 2:\n",
    "                x = x.reshape(-1, d, self.hidden_size)\n",
    "                x, _ = self.rnn1(x)\n",
    "                x = self.pooler(x)\n",
    "                x = x.reshape(-1, 25, self.hidden_size)\n",
    "                x = x.reshape(-1, 5, 5, self.hidden_size)\n",
    "                x = x.permute(0, 2, 1, 3)\n",
    "                x = x.reshape(-1, 5, self.hidden_size)\n",
    "                x_, _ = self.rnn2(x)\n",
    "                x = x + x_\n",
    "\n",
    "            elif self.version == 3:\n",
    "                x = x.reshape(-1, 5, 5, d, self.hidden_size)\n",
    "                x = x.permute(0, 2, 1, 3, 4)\n",
    "                x = x.reshape(-1, 5 * d, self.hidden_size)\n",
    "                x, _ = self.rnn1(x)\n",
    "                x = x.reshape(-1, d, self.hidden_size)\n",
    "                x = self.pooler(x)\n",
    "\n",
    "            x = x.reshape(-1, 5, 5, self.hidden_size)\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "            x = x.reshape(-1, 25, self.hidden_size)\n",
    "\n",
    "\n",
    "            x1 = x[:, 0:5]\n",
    "            x2 = x[:, 5:15]\n",
    "            x3 = x[:, 15:25]\n",
    "\n",
    "        x1 = self.out1(x1)\n",
    "        x2 = self.out2(x2)\n",
    "        x3 = self.out3(x3)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim = 1)\n",
    "        return x\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for SEED in [42, 2024]:\n",
    "        for VERSION in [1, 2, 3]:\n",
    "            \n",
    "            model_weights = {0 :  glob.glob(f'input/stage2_{SEED}_{VERSION}/fold1/*.bin')[0],\n",
    "                             1 :  glob.glob(f'input/stage2_{SEED}_{VERSION}/fold2/*.bin')[0],\n",
    "                             2 :  glob.glob(f'input/stage2_{SEED}_{VERSION}/fold3/*.bin')[0],\n",
    "                             3 :  glob.glob(f'input/stage2_{SEED}_{VERSION}/fold4/*.bin')[0],\n",
    "                             4 :  glob.glob(f'input/stage2_{SEED}_{VERSION}/fold5/*.bin')[0],}\n",
    "            \n",
    "            CustomConfig.seed = SEED\n",
    "            CustomConfig.version = VERSION\n",
    "            args = CustomConfig()\n",
    "\n",
    "            train, folds, detects = preprocess(args)\n",
    "            for i in range(args.n_fold):\n",
    "                seed_everything(args.seed)\n",
    "\n",
    "                train_df, test_df = folds[i]\n",
    "\n",
    "                train_dataset = CustomDataset(args, train_df, detects, mode = 'train')\n",
    "                test_dataset = CustomDataset(args, test_df, detects, mode = 'test')\n",
    "\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size = args.batch_size,\n",
    "                                                           num_workers = args.n_worker,\n",
    "                                                           shuffle = True,\n",
    "                                                           drop_last = True)\n",
    "                test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                          batch_size = args.batch_size,\n",
    "                                                          num_workers = args.n_worker,\n",
    "                                                          shuffle = False,\n",
    "                                                          drop_last = False)\n",
    "\n",
    "                model = CustomModel(args)\n",
    "                model.load_state_dict(torch.load(model_weights[i]))\n",
    "                model.initialize()\n",
    "\n",
    "                model = model.to(args.device)\n",
    "\n",
    "                name = f'/stage2_finetuned_{SEED}_{VERSION}'\n",
    "                args.save_dir = DATA_DIR + name + f'/fold{i+1}'\n",
    "                args.total_steps = int(len(train_df) * args.n_epoch/(args.batch_size * args.iters_to_accumulate))\n",
    "\n",
    "                trainer = CustomTrainer(args, model)\n",
    "                trainer.run(args, train_loader, test_loader)\n",
    "                del model, trainer\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OgsKxoKnbsc_",
    "Mj3ahmv2bniB",
    "LfdZdnCESz3o",
    "JpOm4ClDTQuy",
    "K9NzV_ZYP9fr",
    "5Sj9j4J_K1Dk",
    "JCqQT_2jVSQX"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "060fcd70342c42719fdbacefdeb10258": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20b2fd573377496fba52930dc701c7d2",
      "placeholder": "",
      "style": "IPY_MODEL_639ff3cd2c504ad2a4b6196756324aca",
      "value": "model.safetensors:100%"
     }
    },
    "119fb9b344b74247bf5d9cfd6acd6679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b2fd573377496fba52930dc701c7d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2246e611c2fd429fb6729ae9d446d8d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b585de85dc54284984f14d7c8ae9f77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "639ff3cd2c504ad2a4b6196756324aca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fce1dd1e4904f02b0f019fb460d7e87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5402e9ebaf540818ba13213dabd7b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e480f5c0134fbaac835d49ee065267",
      "placeholder": "",
      "style": "IPY_MODEL_2b585de85dc54284984f14d7c8ae9f77",
      "value": "201M/201M[00:06&lt;00:00,26.8MB/s]"
     }
    },
    "cd8d3da31866406291ed6b6baf95000e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_119fb9b344b74247bf5d9cfd6acd6679",
      "max": 200928946,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2246e611c2fd429fb6729ae9d446d8d3",
      "value": 200928946
     }
    },
    "ee6650e966f44c6096fcdb0dd190bb16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_060fcd70342c42719fdbacefdeb10258",
       "IPY_MODEL_cd8d3da31866406291ed6b6baf95000e",
       "IPY_MODEL_c5402e9ebaf540818ba13213dabd7b01"
      ],
      "layout": "IPY_MODEL_6fce1dd1e4904f02b0f019fb460d7e87"
     }
    },
    "f9e480f5c0134fbaac835d49ee065267": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
